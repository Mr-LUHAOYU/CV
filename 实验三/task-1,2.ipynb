{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7393a9c2279e8651",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 设置OpenAI API密钥\n",
    "clients = OpenAI(\n",
    "    api_key='sk-38d7140e0b844cf387a75a385e28e74f',\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97d61d8e996ae5fd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 初始化CLIP模型\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec02526e9dd4aec6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_path, classes):\n",
    "        self.data_path = data_path\n",
    "        self.classes = classes\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label, class_name in enumerate(classes):\n",
    "            class_path = os.path.join(data_path, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                self.images.append(os.path.join(class_path, img_name))\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.images[idx]))\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be246e089add5b0b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_class_descriptions(classes, method=\"simple\"):\n",
    "    \"\"\"\n",
    "    使用LLM生成类别描述\n",
    "    method: \"simple\" - 简单描述\n",
    "            \"contrastive\" - 对比描述\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        if method == \"simple\":\n",
    "            prompt = f\"Generate a detailed description of what a '{class_name}' looks like, focusing on visual characteristics that would help recognize it in an image. Only keywords are allowed, not sentences, no more than 10 words in total.\"\n",
    "        elif method == \"contrastive\":\n",
    "            other_classes = [c for c in classes if c != class_name]\n",
    "            prompt = f\"Describe how to distinguish a '{class_name}' from {', '.join(other_classes)}. Focus on visual differences. Only keywords are allowed, not sentences, no more than 10 words in total.\"\n",
    "        \n",
    "        response = clients.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        descriptions[class_name] = response.choices[0].message.content\n",
    "    \n",
    "    return descriptions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a2d7a32193048bb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_clip(dataset, text_descriptions=None, number_descriptions=None):\n",
    "    \"\"\"\n",
    "    评估CLIP模型性能\n",
    "    text_descriptions: None - 使用原始类别名称\n",
    "                      dict - 使用生成的描述文本\n",
    "    \"\"\"\n",
    "    if text_descriptions is None:\n",
    "        # 使用原始类别名称\n",
    "        if number_descriptions is None:\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in dataset.classes]).to(device)\n",
    "        else:\n",
    "            text_inputs = torch.cat([clip.tokenize(\n",
    "                f\"a photo of {number_descriptions} {c}{'s' if number_descriptions > 1 else ''}\"\n",
    "            ) for c in dataset.classes]).to(device)\n",
    "    else:\n",
    "        # 使用生成的描述文本\n",
    "        text_inputs = torch.cat([clip.tokenize(d) for d in text_descriptions.values()]).to(device)\n",
    "    \n",
    "    # 计算文本特征\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # 预测所有图像\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    for images, labels in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # 计算图像特征\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 计算相似度\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        preds = similarity.argmax(dim=-1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, cm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29e454601a083170",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "568da2f2280ce553",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 示例数据集 (实际使用时替换为真实数据集)\n",
    "    classes = [\"dog\", \"cat\"]\n",
    "    data_path = \"imgs\"  # 数据集应按照类别分文件夹存放\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = Dataset(data_path, classes)\n",
    "    \n",
    "    # 1. 基线评估 - 使用原始类别名称\n",
    "    base_acc, base_cm = evaluate_clip(dataset)\n",
    "    print(f\"Baseline Accuracy: {base_acc:.4f}\")\n",
    "    # plot_confusion_matrix(base_cm, classes, \"Baseline CLIP Confusion Matrix\")\n",
    "    \n",
    "    # 2. 使用LLM生成的简单描述\n",
    "    simple_descriptions = generate_class_descriptions(classes, method=\"simple\")\n",
    "    simple_acc, simple_cm = evaluate_clip(dataset, simple_descriptions)\n",
    "    print(f\"Simple Descriptions Accuracy: {simple_acc:.4f}\")\n",
    "    # plot_confusion_matrix(simple_cm, classes, \"CLIP with Simple Descriptions\")\n",
    "    \n",
    "    # 3. 使用LLM生成的对比描述\n",
    "    contrastive_descriptions = generate_class_descriptions(classes, method=\"contrastive\")\n",
    "    contrast_acc, contrast_cm = evaluate_clip(dataset, contrastive_descriptions)\n",
    "    print(f\"Contrastive Descriptions Accuracy: {contrast_acc:.4f}\")\n",
    "    # plot_confusion_matrix(contrast_cm, classes, \"CLIP with Contrastive Descriptions\")\n",
    "    \n",
    "    # 结果比较\n",
    "    methods = [\"Baseline\", \"Simple\", \"Contrastive\"]\n",
    "    accuracies = [base_acc, simple_acc, contrast_acc]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(methods, accuracies)\n",
    "    plt.title(\"Comparison of CLIP Performance with Different Text Descriptions\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0, 1.2)\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.02, f\"{acc:.4f}\", ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd2e6050bacc207b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4246f8f430233ecb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_number():\n",
    "    classes = [\"dog\", \"cat\"]\n",
    "    data_path = \"imgs\"  # 数据集应按照类别分文件夹存放\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = Dataset(data_path, classes)\n",
    "    \n",
    "    acc1, cm1 = evaluate_clip(dataset, number_descriptions=1)\n",
    "    acc2, cm2 = evaluate_clip(dataset, number_descriptions=2)\n",
    "    acc3, cm3 = evaluate_clip(dataset, number_descriptions=3)\n",
    "    acc10, cm10 = evaluate_clip(dataset, number_descriptions=10)\n",
    "\n",
    "    # 结果比较\n",
    "    methods = [\"One\", \"Two\", \"Three\", 'Ten']\n",
    "    accuracies = [acc1, acc2, acc3, acc10]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(methods, accuracies)\n",
    "    plt.title(\"Comparison of CLIP Performance with Different Number Descriptions\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0, 1.2)\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.02, f\"{acc:.4f}\", ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "201730993bd31c32",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_number()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cda9ee060fa08d55",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
